{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: A simple diffusion model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OimlcBLxYkqc",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This UNET-style prediction model was originally included as part of the Score-based generative modelling tutorial\n",
    "# by Yang Song et al: https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing\n",
    "\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, scale=30.0):\n",
    "        super().__init__()\n",
    "        # Randomly sample weights during initialization. These weights are fixed\n",
    "        # during optimization and are not trainable.\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)[..., None, None]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
    "        \"\"\"Initialize a time-dependent score-based network.\n",
    "\n",
    "        Args:\n",
    "          marginal_prob_std: A function that takes time t and gives the standard\n",
    "            deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "          channels: The number of channels for feature maps of each resolution.\n",
    "          embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Gaussian random feature embedding layer for time\n",
    "        self.embed = nn.Sequential(\n",
    "            GaussianFourierProjection(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        # Encoding layers where the resolution decreases\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "        self.dense1 = Dense(embed_dim, channels[0])\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "        self.dense2 = Dense(embed_dim, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense3 = Dense(embed_dim, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "        self.dense4 = Dense(embed_dim, channels[3])\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
    "\n",
    "        # Decoding layers where the resolution increases\n",
    "        self.tconv4 = nn.ConvTranspose2d(\n",
    "            channels[3], channels[2], 3, stride=2, bias=False\n",
    "        )\n",
    "        self.dense5 = Dense(embed_dim, channels[2])\n",
    "        self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.tconv3 = nn.ConvTranspose2d(\n",
    "            channels[2] + channels[2],\n",
    "            channels[1],\n",
    "            3,\n",
    "            stride=2,\n",
    "            bias=False,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.dense6 = Dense(embed_dim, channels[1])\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.tconv2 = nn.ConvTranspose2d(\n",
    "            channels[1] + channels[1],\n",
    "            channels[0],\n",
    "            3,\n",
    "            stride=2,\n",
    "            bias=False,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.dense7 = Dense(embed_dim, channels[0])\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
    "\n",
    "        # The swish activation function\n",
    "        self.act = lambda x: x * torch.sigmoid(x)\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Obtain the Gaussian random feature embedding for t\n",
    "        embed = self.act(self.embed(t))\n",
    "        # Encoding path\n",
    "        h1 = self.conv1(x)\n",
    "        ## Incorporate information from t\n",
    "        h1 += self.dense1(embed)\n",
    "        ## Group normalization\n",
    "        h1 = self.gnorm1(h1)\n",
    "        h1 = self.act(h1)\n",
    "        h2 = self.conv2(h1)\n",
    "        h2 += self.dense2(embed)\n",
    "        h2 = self.gnorm2(h2)\n",
    "        h2 = self.act(h2)\n",
    "        h3 = self.conv3(h2)\n",
    "        h3 += self.dense3(embed)\n",
    "        h3 = self.gnorm3(h3)\n",
    "        h3 = self.act(h3)\n",
    "        h4 = self.conv4(h3)\n",
    "        h4 += self.dense4(embed)\n",
    "        h4 = self.gnorm4(h4)\n",
    "        h4 = self.act(h4)\n",
    "\n",
    "        # Decoding path\n",
    "        h = self.tconv4(h4)\n",
    "        ## Skip connection from the encoding path\n",
    "        h += self.dense5(embed)\n",
    "        h = self.tgnorm4(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv3(torch.cat([h, h3], dim=1))\n",
    "        h += self.dense6(embed)\n",
    "        h = self.tgnorm3(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "        h += self.dense7(embed)\n",
    "        h = self.tgnorm2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "        # Normalize output\n",
    "        h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ExponentialMovingAverage implementation as used in pytorch vision\n",
    "# https://github.com/pytorch/vision/blob/main/references/classification/utils.py#L159\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) Soumith Chintala 2016,\n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# * Redistributions of source code must retain the above copyright notice, this\n",
    "#   list of conditions and the following disclaimer.\n",
    "\n",
    "# * Redistributions in binary form must reproduce the above copyright notice,\n",
    "#   this list of conditions and the following disclaimer in the documentation\n",
    "#   and/or other materials provided with the distribution.\n",
    "\n",
    "# * Neither the name of the copyright holder nor the names of its\n",
    "#   contributors may be used to endorse or promote products derived from\n",
    "#   this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "\n",
    "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
    "    \"\"\"Maintains moving averages of model parameters using an exponential decay.\n",
    "    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``\n",
    "    `torch.optim.swa_utils.AveragedModel <https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies>`_\n",
    "    is used to compute the EMA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay, device=\"cpu\"):\n",
    "        def ema_avg(avg_model_param, model_param, num_averaged):\n",
    "            return decay * avg_model_param + (1 - decay) * model_param\n",
    "\n",
    "        super().__init__(model, device, ema_avg, use_buffers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "645d91e4bb974b1196be61b5077c9dc5",
      "78dc714c7aa347fb9fc41abf420222d9",
      "c1260f271df547fbb2a158ff6b3a3ff4",
      "e7313fdbb70442f4867644dfc85c3bcc",
      "a501588b5eb0494996dfb136565365ca",
      "89c68eded05d441daf94d145addb5ece",
      "2bffd3855f5744f588d5be1e5c4aed3e",
      "3b61ee9c62994863b718c086d4182f44",
      "8b905c5b2ad846ca837bd20cce2bf094",
      "b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "aca161ff9f4b4a20b1457a8ee864f150"
     ]
    },
    "id": "mcoxR2ajYkqe",
    "outputId": "1f39bd8e-e78c-42e6-89cc-f1df34bdbdea"
   },
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "\n",
    "    def __init__(self, network, T=100, beta_1=1e-4, beta_T=2e-2):\n",
    "        \"\"\"\n",
    "        Initialize Denoising Diffusion Probabilistic Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network: nn.Module\n",
    "            The inner neural network used by the diffusion process. Typically a Unet.\n",
    "        beta_1: float\n",
    "            beta_t value at t=1\n",
    "        beta_T: [float]\n",
    "            beta_t value at t=T (last step)\n",
    "        T: int\n",
    "            The number of diffusion steps.\n",
    "        \"\"\"\n",
    "\n",
    "        super(DDPM, self).__init__()\n",
    "\n",
    "        # Normalize time input before evaluating neural network\n",
    "        # Reshape input into image format and normalize time value before sending it to network model\n",
    "        self._network = network\n",
    "        self.network = lambda x, t: (\n",
    "            self._network(x.reshape(-1, 1, 28, 28), (t.squeeze() / T))\n",
    "        ).reshape(-1, 28 * 28)\n",
    "\n",
    "        # Total number of time steps\n",
    "        self.T = T\n",
    "\n",
    "        # Registering as buffers to ensure they get transferred to the GPU automatically\n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_1, beta_T, T + 1))\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
    "\n",
    "    def forward_diffusion(self, x0, t, epsilon):\n",
    "        \"\"\"\n",
    "        q(x_t | x_0)\n",
    "        Forward diffusion from an input datapoint x0 to an xt at timestep t, provided a N(0,1) noise sample epsilon.\n",
    "        Note that we can do this operation in a single step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            x value at t=0 (an input image)\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t\n",
    "        \"\"\"\n",
    "\n",
    "        mean = torch.sqrt(self.alpha_bar[t]) * x0\n",
    "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
    "\n",
    "        return mean + std * epsilon\n",
    "\n",
    "    def reverse_diffusion(self, xt, t, epsilon):\n",
    "        \"\"\"\n",
    "        p(x_{t-1} | x_t)\n",
    "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xt: torch.tensor\n",
    "            x value at step t\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t-1\n",
    "        \"\"\"\n",
    "\n",
    "        mean = (\n",
    "            1.0\n",
    "            / torch.sqrt(self.alpha[t])\n",
    "            * (\n",
    "                xt\n",
    "                - (self.beta[t])\n",
    "                / torch.sqrt(1 - self.alpha_bar[t])\n",
    "                * self.network(xt, t)\n",
    "            )\n",
    "        )\n",
    "        std = torch.where(\n",
    "            t > 0,\n",
    "            torch.sqrt(\n",
    "                ((1 - self.alpha_bar[t - 1]) / (1 - self.alpha_bar[t])) * self.beta[t]\n",
    "            ),\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        return mean + std * epsilon\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape):\n",
    "        \"\"\"\n",
    "        Sample from diffusion model (Algorithm 2 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: tuple\n",
    "            Specify shape of sampled output. For MNIST: (nsamples, 28*28)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            sampled image\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample xT: Gaussian noise\n",
    "        xT = torch.randn(shape).to(self.beta.device)\n",
    "\n",
    "        xt = xT\n",
    "        for t in range(self.T, 0, -1):\n",
    "            noise = torch.randn_like(xT) if t > 1 else 0\n",
    "            t = torch.tensor(t).expand(xt.shape[0], 1).to(self.beta.device)\n",
    "            xt = self.reverse_diffusion(xt, t, noise)\n",
    "\n",
    "        return xt\n",
    "\n",
    "    def elbo_simple(self, x0):\n",
    "        \"\"\"\n",
    "        ELBO training objective (Algorithm 1 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            ELBO value\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample time step t\n",
    "        t = torch.randint(1, self.T, (x0.shape[0], 1)).to(x0.device)\n",
    "\n",
    "        # Sample noise\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        # TODO: Forward diffusion to produce image at step t\n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "\n",
    "        return -nn.MSELoss(reduction=\"mean\")(epsilon, self.network(xt, t))\n",
    "\n",
    "    def loss(self, x0):\n",
    "        \"\"\"\n",
    "        Loss function. Just the negative of the ELBO.\n",
    "        \"\"\"\n",
    "        return -self.elbo_simple(x0).mean()\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    dataloader,\n",
    "    epochs,\n",
    "    device,\n",
    "    ema=True,\n",
    "    per_epoch_callback=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Pytorch model\n",
    "    optimizer: optim.Optimizer\n",
    "        Pytorch optimizer to be used for training\n",
    "    scheduler: optim.LRScheduler\n",
    "        Pytorch learning rate scheduler\n",
    "    dataloader: utils.DataLoader\n",
    "        Pytorch dataloader\n",
    "    epochs: int\n",
    "        Number of epochs to train\n",
    "    device: torch.device\n",
    "        Pytorch device specification\n",
    "    ema: Boolean\n",
    "        Whether to activate Exponential Model Averaging\n",
    "    per_epoch_callback: function\n",
    "        Called at the end of every epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup progress bar\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    if ema:\n",
    "        ema_global_step_counter = 0\n",
    "        ema_steps = 10\n",
    "        ema_adjust = dataloader.batch_size * ema_steps / epochs\n",
    "        ema_decay = 1.0 - 0.995\n",
    "        ema_alpha = min(1.0, (1.0 - ema_decay) * ema_adjust)\n",
    "        ema_model = ExponentialMovingAverage(\n",
    "            model, device=device, decay=1.0 - ema_alpha\n",
    "        )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Switch to train mode\n",
    "        model.train()\n",
    "\n",
    "        global_step_counter = 0\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"⠀{loss.item():12.4f}\",\n",
    "                epoch=f\"{epoch+1}/{epochs}\",\n",
    "                lr=f\"{scheduler.get_last_lr()[0]:.2E}\",\n",
    "            )\n",
    "            progress_bar.update()\n",
    "\n",
    "            if ema:\n",
    "                ema_global_step_counter += 1\n",
    "                if ema_global_step_counter % ema_steps == 0:\n",
    "                    ema_model.update_parameters(model)\n",
    "\n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(ema_model.module if ema else model)\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "\n",
    "# Parameters\n",
    "T = 1000\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "# Rather than treating MNIST images as discrete objects, as done in Ho et al 2020,\n",
    "# we here treat them as continuous input data, by dequantizing the pixel values (adding noise to the input data)\n",
    "# Also note that we map the 0..255 pixel values to [-1, 1], and that we process the 28x28 pixel values as a flattened 784 tensor.\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(\n",
    "            lambda x: x + torch.rand(x.shape) / 255\n",
    "        ),  # Dequantize pixel values\n",
    "        transforms.Lambda(lambda x: (x - 0.5) * 2.0),  # Map from [0,1] -> [-1, -1]\n",
    "        transforms.Lambda(lambda x: x.flatten()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download and transform train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Construct Unet\n",
    "# The original ScoreNet expects a function with std for all the\n",
    "# different noise levels, such that the output can be rescaled.\n",
    "# Since we are predicting the noise (rather than the score), we\n",
    "# ignore this rescaling and just set std=1 for all t.\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = DDPM(mnist_unet, T=T).to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "\n",
    "def reporter(model):\n",
    "    \"\"\"Callback function used for plotting images during training\"\"\"\n",
    "\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nsamples = 10\n",
    "        samples = model.sample((nsamples, 28 * 28)).cpu()\n",
    "\n",
    "        # Map pixel values back from [-1,1] to [0,1]\n",
    "        samples = (samples + 1) / 2\n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "\n",
    "        # Plot in grid\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=nsamples)\n",
    "        plt.gca().set_axis_off()\n",
    "        plt.imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Call training loop\n",
    "train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    dataloader_train,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    "    ema=True,\n",
    "    per_epoch_callback=reporter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction 1: Converting the MNIST DDPM template to Flow Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Flow Matching model using the conditional OT / linear Gaussian path.\n",
    "    \"\"\"\n",
    "    def __init__(self, network):\n",
    "        super().__init__()\n",
    "        self.network = network  \n",
    "        \n",
    "    def velocity(self, x_flat, t):\n",
    "        if t.ndim == 2:\n",
    "            t_in = t.squeeze(1)\n",
    "        else:\n",
    "            t_in = t\n",
    "        v = self.network(x_flat.reshape(-1, 1, 28, 28), t_in).reshape(-1, 28*28)\n",
    "        return v\n",
    "\n",
    "    def loss(self, x1_flat):\n",
    "        B = x1_flat.shape[0]\n",
    "        device = x1_flat.device\n",
    "\n",
    "        x0 = torch.randn_like(x1_flat)\n",
    "        t = torch.rand(B, 1, device=device)\n",
    "        xt = (1.0 - t) * x0 + t * x1_flat\n",
    "        v_target = x1_flat - x0\n",
    "        v_pred = self.velocity(xt, t)\n",
    "\n",
    "        return F.mse_loss(v_pred, v_target, reduction=\"mean\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, nsamples, n_steps=50):\n",
    "        device = next(self.parameters()).device\n",
    "        x = torch.randn(nsamples, 28*28, device=device)\n",
    "\n",
    "        t_grid = torch.linspace(0.0, 1.0, n_steps + 1, device=device)\n",
    "        for k in range(n_steps):\n",
    "            t_k = t_grid[k].expand(nsamples, 1)\n",
    "            dt = (t_grid[k+1] - t_grid[k]).item()\n",
    "            x = x + dt * self.velocity(x, t_k)\n",
    "\n",
    "        return x\n",
    "\n",
    "fm_unet = ScoreNet((lambda t: torch.ones(1).to(device))).to(device)\n",
    "\n",
    "fm_model = FlowMatchingModel(fm_unet).to(device)\n",
    "\n",
    "fm_optimizer = torch.optim.Adam(fm_model.parameters(), lr=learning_rate)\n",
    "fm_scheduler = torch.optim.lr_scheduler.ExponentialLR(fm_optimizer, 0.9999)\n",
    "\n",
    "def train_flow_matching(model, optimizer, scheduler, dataloader, epochs, device, per_epoch_callback=None):\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training (Flow Matching)\")\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x1, _ in dataloader:\n",
    "            x1 = x1.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"⠀{loss.item():12.4f}\",\n",
    "                                     epoch=f\"{epoch+1}/{epochs}\",\n",
    "                                     lr=f\"{scheduler.get_last_lr()[0]:.2E}\")\n",
    "            progress_bar.update()\n",
    "\n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(model)\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "def reporter_flow_matching(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        nsamples = 10\n",
    "        samples = model.sample(nsamples=nsamples, n_steps=50).cpu()\n",
    "        samples = (samples + 1) / 2\n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=nsamples)\n",
    "        plt.gca().set_axis_off()\n",
    "        plt.imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "epochs_full = epochs\n",
    "batch_size_full = batch_size\n",
    "\n",
    "dataloader_full = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=batch_size_full,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# DDPM training\n",
    "ddpm_full = DDPM(ScoreNet(lambda t: torch.ones(1).to(device)), T=T).to(device)\n",
    "opt_d_full = torch.optim.Adam(ddpm_full.parameters(), lr=learning_rate)\n",
    "sch_d_full = torch.optim.lr_scheduler.ExponentialLR(opt_d_full, 0.9999)\n",
    "\n",
    "loss_ddpm_full = train(\n",
    "    ddpm_full,\n",
    "    opt_d_full,\n",
    "    sch_d_full,\n",
    "    dataloader_full,\n",
    "    epochs=epochs_full,\n",
    "    device=device,\n",
    "    ema=True,\n",
    "    per_epoch_callback=None,\n",
    ")\n",
    "\n",
    "# Flow Matching training\n",
    "fm_full = FlowMatchingModel(ScoreNet(lambda t: torch.ones(1).to(device)).to(device)).to(device)\n",
    "opt_f_full = torch.optim.Adam(fm_full.parameters(), lr=learning_rate)\n",
    "sch_f_full = torch.optim.lr_scheduler.ExponentialLR(opt_f_full, 0.9999)\n",
    "\n",
    "loss_fm_full = train_flow_matching(\n",
    "    fm_full,\n",
    "    opt_f_full,\n",
    "    sch_f_full,\n",
    "    dataloader_full,\n",
    "    epochs=epochs_full,\n",
    "    device=device,\n",
    "    per_epoch_callback=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Plot 1: Training loss comparison with smoothing\n",
    "window = 100\n",
    "ddpm_smooth = np.convolve(loss_ddpm_full, np.ones(window)/window, mode='valid')\n",
    "fm_smooth = np.convolve(loss_fm_full, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(ddpm_smooth, label=\"DDPM\", linewidth=2, alpha=0.8)\n",
    "axes[0].plot(fm_smooth, label=\"Flow Matching\", linewidth=2, alpha=0.8)\n",
    "axes[0].set_title(\"Training Loss Comparison\", fontsize=11)\n",
    "axes[0].set_xlabel(\"Training Step\", fontsize=10)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=10)\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: DDPM samples\n",
    "ddpm_full.eval()\n",
    "with torch.no_grad():\n",
    "    ddpm_samples = ddpm_full.sample((16, 28 * 28)).cpu()\n",
    "    ddpm_samples = ((ddpm_samples + 1) / 2).clamp(0, 1)\n",
    "    ddpm_grid_plot2 = utils.make_grid(ddpm_samples.reshape(-1, 1, 28, 28), nrow=4, padding=1, pad_value=1.0)\n",
    "\n",
    "axes[1].imshow(transforms.functional.to_pil_image(ddpm_grid_plot2), cmap=\"gray\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"DDPM Samples\", fontsize=11)\n",
    "\n",
    "# Plot 3: Flow Matching samples\n",
    "fm_full.eval()\n",
    "with torch.no_grad():\n",
    "    fm_samples = fm_full.sample(nsamples=16, n_steps=100).cpu()\n",
    "    fm_samples = ((fm_samples + 1) / 2).clamp(0, 1)\n",
    "    fm_grid_plot3 = utils.make_grid(fm_samples.reshape(-1, 1, 28, 28), nrow=4, padding=1, pad_value=1.0)\n",
    "\n",
    "axes[2].imshow(transforms.functional.to_pil_image(fm_grid_plot3), cmap=\"gray\")\n",
    "axes[2].axis(\"off\")\n",
    "axes[2].set_title(\"Flow Matching Samples\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure Mode Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Too few ODE steps\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Test different numbers of ODE integration steps\n",
    "step_counts = [1, 5, 10, 20, 200, 1000]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "fm_full.eval()\n",
    "with torch.no_grad():\n",
    "    for i, n_steps in enumerate(step_counts):\n",
    "        samples = fm_full.sample(nsamples=16, n_steps=n_steps).cpu()\n",
    "        samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "        \n",
    "        axes[i].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"FM with {n_steps} ODE steps\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Effect of ODE Integration Steps on Sample Quality\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Inadequate training (early stopping)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Train Flow Matching models with different numbers of epochs\n",
    "epoch_counts = [1, 5, 10, 20, 50, 100]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, n_epochs in enumerate(epoch_counts):\n",
    "    fm_test = FlowMatchingModel(ScoreNet(lambda t: torch.ones(1).to(device)).to(device)).to(device)\n",
    "    opt_test = torch.optim.Adam(fm_test.parameters(), lr=learning_rate)\n",
    "    sch_test = torch.optim.lr_scheduler.ExponentialLR(opt_test, 0.9999)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    _ = train_flow_matching(\n",
    "        fm_test,\n",
    "        opt_test,\n",
    "        sch_test,\n",
    "        dataloader_test,\n",
    "        epochs=n_epochs,\n",
    "        device=device,\n",
    "        per_epoch_callback=None,\n",
    "    )\n",
    "    \n",
    "    fm_test.eval()\n",
    "    with torch.no_grad():\n",
    "        samples = fm_test.sample(nsamples=16, n_steps=50).cpu()\n",
    "        samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "        \n",
    "        axes[i].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"FM trained for {n_epochs} epochs\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Effect of Training Duration on Sample Quality\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. High learning rate instability\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Train Flow Matching models with different learning rates\n",
    "learning_rates = [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 1e-5]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "training_epochs = 30\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    fm_test = FlowMatchingModel(ScoreNet(lambda t: torch.ones(1).to(device)).to(device)).to(device)\n",
    "    opt_test = torch.optim.Adam(fm_test.parameters(), lr=lr)\n",
    "    sch_test = torch.optim.lr_scheduler.ExponentialLR(opt_test, 0.9999)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    _ = train_flow_matching(\n",
    "        fm_test,\n",
    "        opt_test,\n",
    "        sch_test,\n",
    "        dataloader_test,\n",
    "        epochs=training_epochs,\n",
    "        device=device,\n",
    "        per_epoch_callback=None,\n",
    "    )\n",
    "    \n",
    "    fm_test.eval()\n",
    "    with torch.no_grad():\n",
    "        samples = fm_test.sample(nsamples=16, n_steps=50).cpu()\n",
    "        samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "        \n",
    "        axes[i].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"FM with LR={lr:.0e}\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Effect of Learning Rate on Sample Quality\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fm_failure_learning_rate.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction 2: Latent Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Convolutional encoder that compresses 28x28 images to a latent vector.\"\"\"\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, latent_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Convolutional decoder that reconstructs images from latent vectors.\"\"\"\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.fc = nn.Linear(latent_dim, 128 * 4 * 4)\n",
    "        \n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        h = self.fc(z)\n",
    "        h = h.view(-1, 128, 4, 4)\n",
    "        return self.deconv_layers(h)\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Standard Autoencoder for learning latent representations.\"\"\"\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode image to latent space.\"\"\"\n",
    "        if x.ndim == 2:\n",
    "            x = x.reshape(-1, 1, 28, 28)\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vector to image.\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass: encode then decode.\"\"\"\n",
    "        if x.ndim == 2:\n",
    "            x = x.reshape(-1, 1, 28, 28)\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, z\n",
    "    \n",
    "    def loss(self, x):\n",
    "        \"\"\"Reconstruction loss (MSE).\"\"\"\n",
    "        if x.ndim == 2:\n",
    "            x = x.reshape(-1, 1, 28, 28)\n",
    "        x_recon, _ = self.forward(x)\n",
    "        return F.mse_loss(x_recon, x, reduction=\"mean\")\n",
    "\n",
    "\n",
    "def train_autoencoder(model, optimizer, scheduler, dataloader, epochs, device, per_epoch_callback=None):\n",
    "    \"\"\"Training loop for the autoencoder.\"\"\"\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training Autoencoder\")\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            if x.ndim == 2:\n",
    "                x = x.reshape(-1, 1, 28, 28)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            loss_history.append(loss.item())\n",
    "            \n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"⠀{loss.item():12.6f}\",\n",
    "                epoch=f\"{epoch+1}/{epochs}\",\n",
    "                lr=f\"{scheduler.get_last_lr()[0]:.2E}\"\n",
    "            )\n",
    "            progress_bar.update()\n",
    "        \n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(model)\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A smaller UNet-style network for operating in the latent space.\n",
    "    Since the latent space is much smaller (32 dims vs 784), we use a simpler architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=32, hidden_dim=256, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.time_embed = nn.Sequential(\n",
    "            GaussianFourierProjection(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.input_proj = nn.Linear(latent_dim, hidden_dim)\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.output_proj = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, z, t):\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        h = self.input_proj(z)\n",
    "        \n",
    "        h = self.norm1(h + self.block1(h + t_emb))\n",
    "        h = self.norm2(h + self.block2(h + t_emb))\n",
    "        h = self.norm3(h + self.block3(h + t_emb))\n",
    "        \n",
    "        return self.output_proj(h)\n",
    "\n",
    "\n",
    "class LatentDDPM(nn.Module):\n",
    "    \"\"\"\n",
    "    DDPM operating in the latent space of a pre-trained autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, autoencoder, network, T=1000, beta_1=1e-4, beta_T=2e-2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.autoencoder = autoencoder\n",
    "        for param in self.autoencoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.network = network\n",
    "        self.T = T\n",
    "        \n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_1, beta_T, T + 1))\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
    "    \n",
    "    def forward_diffusion(self, z0, t, epsilon):\n",
    "        \"\"\"Forward diffusion in latent space: q(z_t | z_0)\"\"\"\n",
    "        mean = torch.sqrt(self.alpha_bar[t]) * z0\n",
    "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
    "        return mean + std * epsilon\n",
    "    \n",
    "    def reverse_diffusion(self, zt, t, epsilon):\n",
    "        \"\"\"Reverse diffusion step in latent space: p(z_{t-1} | z_t)\"\"\"\n",
    "        t_normalized = t.squeeze().float() / self.T\n",
    "        pred_noise = self.network(zt, t_normalized)\n",
    "        \n",
    "        mean = (\n",
    "            1.0 / torch.sqrt(self.alpha[t]) * (\n",
    "                zt - (self.beta[t]) / torch.sqrt(1 - self.alpha_bar[t]) * pred_noise\n",
    "            )\n",
    "        )\n",
    "        std = torch.where(\n",
    "            t > 0,\n",
    "            torch.sqrt(((1 - self.alpha_bar[t - 1]) / (1 - self.alpha_bar[t])) * self.beta[t]),\n",
    "            torch.zeros_like(t, dtype=torch.float32),\n",
    "        )\n",
    "        \n",
    "        return mean + std * epsilon\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, nsamples):\n",
    "        \"\"\"\n",
    "        Sample from latent DDPM and decode to image space.\n",
    "        \"\"\"\n",
    "        device = self.beta.device\n",
    "        latent_dim = self.autoencoder.latent_dim\n",
    "        \n",
    "        zt = torch.randn(nsamples, latent_dim, device=device)\n",
    "        \n",
    "        for t in range(self.T, 0, -1):\n",
    "            noise = torch.randn_like(zt) if t > 1 else torch.zeros_like(zt)\n",
    "            t_tensor = torch.tensor(t, device=device).expand(nsamples, 1)\n",
    "            zt = self.reverse_diffusion(zt, t_tensor, noise)\n",
    "        \n",
    "        self.autoencoder.eval()\n",
    "        x_decoded = self.autoencoder.decode(zt)\n",
    "        \n",
    "        return x_decoded.reshape(nsamples, -1)\n",
    "    \n",
    "    def loss(self, x):\n",
    "        \"\"\"\n",
    "        ELBO training loss in latent space.\n",
    "        \"\"\"\n",
    "        self.autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            z0 = self.autoencoder.encode(x.reshape(-1, 1, 28, 28))\n",
    "        \n",
    "        t = torch.randint(1, self.T, (z0.shape[0], 1), device=x.device)\n",
    "        \n",
    "        epsilon = torch.randn_like(z0)\n",
    "        \n",
    "        zt = self.forward_diffusion(z0, t, epsilon)\n",
    "        \n",
    "        t_normalized = t.squeeze().float() / self.T\n",
    "        epsilon_pred = self.network(zt, t_normalized)\n",
    "        \n",
    "        return F.mse_loss(epsilon_pred, epsilon, reduction=\"mean\")\n",
    "\n",
    "\n",
    "def train_latent_ddpm(model, optimizer, scheduler, dataloader, epochs, device, per_epoch_callback=None):\n",
    "    \"\"\"Training loop for Latent DDPM.\"\"\"\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training Latent DDPM\")\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            loss_history.append(loss.item())\n",
    "            \n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"⠀{loss.item():12.6f}\",\n",
    "                epoch=f\"{epoch+1}/{epochs}\",\n",
    "                lr=f\"{scheduler.get_last_lr()[0]:.2E}\"\n",
    "            )\n",
    "            progress_bar.update()\n",
    "        \n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(model)\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Shared parameters\n",
    "comparison_epochs = 50\n",
    "comparison_batch_size = 256\n",
    "ae_latent_dim = 32\n",
    "ae_epochs = 30\n",
    "\n",
    "comparison_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=comparison_batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Train Standard DDPM (pixel space)\n",
    "\n",
    "standard_ddpm = DDPM(\n",
    "    ScoreNet(lambda t: torch.ones(1).to(device)), \n",
    "    T=T\n",
    ").to(device)\n",
    "\n",
    "standard_opt = torch.optim.Adam(standard_ddpm.parameters(), lr=learning_rate)\n",
    "standard_sch = torch.optim.lr_scheduler.ExponentialLR(standard_opt, 0.9999)\n",
    "\n",
    "standard_loss_history = train(\n",
    "    standard_ddpm,\n",
    "    standard_opt,\n",
    "    standard_sch,\n",
    "    comparison_dataloader,\n",
    "    epochs=comparison_epochs,\n",
    "    device=device,\n",
    "    ema=True,\n",
    "    per_epoch_callback=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Latent DDPM (latent space)\n",
    "\n",
    "fresh_autoencoder = Autoencoder(latent_dim=ae_latent_dim).to(device)\n",
    "fresh_ae_opt = torch.optim.Adam(fresh_autoencoder.parameters(), lr=learning_rate)\n",
    "fresh_ae_sch = torch.optim.lr_scheduler.ExponentialLR(fresh_ae_opt, 0.9999)\n",
    "\n",
    "fresh_ae_loss = train_autoencoder(\n",
    "    fresh_autoencoder,\n",
    "    fresh_ae_opt,\n",
    "    fresh_ae_sch,\n",
    "    comparison_dataloader,\n",
    "    epochs=ae_epochs,\n",
    "    device=device,\n",
    "    per_epoch_callback=None,\n",
    ")\n",
    "\n",
    "# Then train latent DDPM\n",
    "fresh_latent_unet = LatentUNet(latent_dim=ae_latent_dim, hidden_dim=256, embed_dim=128).to(device)\n",
    "fresh_latent_ddpm = LatentDDPM(fresh_autoencoder, fresh_latent_unet, T=T).to(device)\n",
    "\n",
    "fresh_latent_opt = torch.optim.Adam(fresh_latent_unet.parameters(), lr=learning_rate)\n",
    "fresh_latent_sch = torch.optim.lr_scheduler.ExponentialLR(fresh_latent_opt, 0.9999)\n",
    "\n",
    "fresh_latent_loss = train_latent_ddpm(\n",
    "    fresh_latent_ddpm,\n",
    "    fresh_latent_opt,\n",
    "    fresh_latent_sch,\n",
    "    comparison_dataloader,\n",
    "    epochs=comparison_epochs,\n",
    "    device=device,\n",
    "    per_epoch_callback=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Plot 1: Training loss comparison with smoothing\n",
    "window = 100\n",
    "standard_smooth = np.convolve(standard_loss_history, np.ones(window)/window, mode='valid')\n",
    "latent_smooth = np.convolve(fresh_latent_loss, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(standard_smooth, label=\"Standard DDPM\", linewidth=2, alpha=0.8)\n",
    "axes[0].plot(latent_smooth, label=\"Latent DDPM\", linewidth=2, alpha=0.8)\n",
    "axes[0].set_title(\"Training Loss Comparison\", fontsize=11)\n",
    "axes[0].set_xlabel(\"Training Step\", fontsize=10)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=10)\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Standard DDPM samples\n",
    "standard_ddpm.eval()\n",
    "with torch.no_grad():\n",
    "    standard_samples = standard_ddpm.sample((16, 28 * 28)).cpu()\n",
    "    standard_samples = ((standard_samples + 1) / 2).clamp(0, 1)\n",
    "    standard_grid = utils.make_grid(standard_samples.reshape(-1, 1, 28, 28), nrow=4, padding=1, pad_value=1.0)\n",
    "\n",
    "axes[1].imshow(transforms.functional.to_pil_image(standard_grid), cmap=\"gray\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Standard DDPM Samples\", fontsize=11)\n",
    "\n",
    "# Plot 3: Latent DDPM samples\n",
    "fresh_latent_ddpm.eval()\n",
    "with torch.no_grad():\n",
    "    latent_samples = fresh_latent_ddpm.sample(16).cpu()\n",
    "    latent_samples = ((latent_samples + 1) / 2).clamp(0, 1)\n",
    "    latent_grid = utils.make_grid(latent_samples.reshape(-1, 1, 28, 28), nrow=4, padding=1, pad_value=1.0)\n",
    "\n",
    "axes[2].imshow(transforms.functional.to_pil_image(latent_grid), cmap=\"gray\")\n",
    "axes[2].axis(\"off\")\n",
    "axes[2].set_title(\"Latent DDPM Samples\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure Mode Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Too few diffusion steps during sampling\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "step_counts = [10, 50, 100, 250, 500, 1000]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "fresh_latent_ddpm.eval()\n",
    "for i, n_steps in enumerate(step_counts):\n",
    "    with torch.no_grad():\n",
    "        device_ddpm = fresh_latent_ddpm.beta.device\n",
    "        latent_dim_ddpm = fresh_latent_ddpm.autoencoder.latent_dim\n",
    "        nsamples = 16\n",
    "        \n",
    "        zt = torch.randn(nsamples, latent_dim_ddpm, device=device_ddpm)\n",
    "        \n",
    "        step_size = fresh_latent_ddpm.T // n_steps\n",
    "        for step_idx in range(n_steps):\n",
    "            t = max(1, fresh_latent_ddpm.T - step_idx * step_size)\n",
    "            noise = torch.randn_like(zt) if t > 1 else torch.zeros_like(zt)\n",
    "            t_tensor = torch.tensor(t, device=device_ddpm).expand(nsamples, 1)\n",
    "            zt = fresh_latent_ddpm.reverse_diffusion(zt, t_tensor, noise)\n",
    "        \n",
    "        x_decoded = fresh_latent_ddpm.autoencoder.decode(zt)\n",
    "        samples = x_decoded.reshape(nsamples, -1).cpu()\n",
    "        samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "        \n",
    "        axes[i].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"Latent DDPM with {n_steps} steps\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Effect of Diffusion Steps on Sample Quality\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Wrong initial noise scale in latent space\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Test different noise scales for initial latent sampling\n",
    "noise_scales = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "fresh_latent_ddpm.eval()\n",
    "for i, scale in enumerate(noise_scales):\n",
    "    with torch.no_grad():\n",
    "        device_ddpm = fresh_latent_ddpm.beta.device\n",
    "        latent_dim_ddpm = fresh_latent_ddpm.autoencoder.latent_dim\n",
    "        nsamples = 16\n",
    "        \n",
    "        zt = torch.randn(nsamples, latent_dim_ddpm, device=device_ddpm) * scale\n",
    "        \n",
    "        for t in range(fresh_latent_ddpm.T, 0, -1):\n",
    "            noise = torch.randn_like(zt) if t > 1 else torch.zeros_like(zt)\n",
    "            t_tensor = torch.tensor(t, device=device_ddpm).expand(nsamples, 1)\n",
    "            zt = fresh_latent_ddpm.reverse_diffusion(zt, t_tensor, noise)\n",
    "        \n",
    "        x_decoded = fresh_latent_ddpm.autoencoder.decode(zt)\n",
    "        samples = x_decoded.reshape(nsamples, -1).cpu()\n",
    "        samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "        \n",
    "        axes[i].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"Initial noise scale={scale:.1f}\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Effect of Initial Noise Scale in Latent Space\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction 3: Continuous-Time SDE Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPSDE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variance Preserving SDE (continuous-time limit of DDPM).\n",
    "    \"\"\"\n",
    "    def __init__(self, network, beta_min=0.1, beta_max=20.0, T=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._network = network\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.T = T\n",
    "    \n",
    "    def beta(self, t):\n",
    "        \"\"\"Linear noise schedule: beta(t) = beta_min + t * (beta_max - beta_min)\"\"\"\n",
    "        return self.beta_min + t * (self.beta_max - self.beta_min)\n",
    "    \n",
    "    def integral_beta(self, t):\n",
    "        \"\"\"Integral of beta from 0 to t: integral_0^t beta(s) ds\"\"\"\n",
    "        return self.beta_min * t + 0.5 * t**2 * (self.beta_max - self.beta_min)\n",
    "    \n",
    "    def marginal_prob_mean_coeff(self, t):\n",
    "        \"\"\"Mean coefficient for p_{0t}(x(t)|x(0)): exp(-0.5 * integral_0^t beta(s) ds)\"\"\"\n",
    "        return torch.exp(-0.5 * self.integral_beta(t))\n",
    "    \n",
    "    def marginal_prob_std(self, t):\n",
    "        \"\"\"Standard deviation for p_{0t}(x(t)|x(0)): sqrt(1 - exp(-integral_0^t beta(s) ds))\"\"\"\n",
    "        return torch.sqrt(1.0 - torch.exp(-self.integral_beta(t)))\n",
    "    \n",
    "    def score(self, x, t):\n",
    "        \"\"\"\n",
    "        Score network output, properly normalized.\n",
    "        \"\"\"\n",
    "        if t.ndim == 2:\n",
    "            t = t.squeeze(1)\n",
    "        \n",
    "        t_normalized = t / self.T\n",
    "        x_img = x.reshape(-1, 1, 28, 28)\n",
    "        \n",
    "        output = self._network(x_img, t_normalized).reshape(-1, 28 * 28)\n",
    "        \n",
    "        std = self.marginal_prob_std(t)[:, None]\n",
    "        return -output / std\n",
    "    \n",
    "    def forward_sample(self, x0, t, epsilon):\n",
    "        \"\"\"\n",
    "        Sample from p_{0t}(x(t) | x(0)) using the closed-form marginal.\n",
    "                \"\"\"\n",
    "        mean_coeff = self.marginal_prob_mean_coeff(t)\n",
    "        std = self.marginal_prob_std(t)\n",
    "        \n",
    "        if mean_coeff.ndim == 1:\n",
    "            mean_coeff = mean_coeff[:, None]\n",
    "        if std.ndim == 1:\n",
    "            std = std[:, None]\n",
    "        \n",
    "        return mean_coeff * x0 + std * epsilon\n",
    "    \n",
    "    def loss(self, x0):\n",
    "        \"\"\"\n",
    "        Denoising score matching loss (continuous-time version).\n",
    "        \"\"\"\n",
    "        batch_size = x0.shape[0]\n",
    "        device = x0.device\n",
    "        \n",
    "        eps = 1e-5\n",
    "        t = torch.rand(batch_size, device=device) * (self.T - eps) + eps\n",
    "        \n",
    "        epsilon = torch.randn_like(x0)\n",
    "        \n",
    "        xt = self.forward_sample(x0, t, epsilon)\n",
    "        \n",
    "        score_pred = self.score(xt, t)\n",
    "        \n",
    "        std = self.marginal_prob_std(t)[:, None]\n",
    "        score_true = -epsilon / std\n",
    "        \n",
    "        loss = torch.mean(torch.sum((score_pred - score_true)**2 * std**2, dim=1))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_sde(self, shape, n_steps=1000):\n",
    "        \"\"\"\n",
    "        Sample using the reverse-time SDE (Euler-Maruyama).\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        x = torch.randn(shape, device=device)\n",
    "        \n",
    "        dt = -self.T / n_steps\n",
    "        \n",
    "        for i in range(n_steps):\n",
    "            t = self.T - i * self.T / n_steps\n",
    "            t_tensor = torch.full((shape[0],), t, device=device)\n",
    "            \n",
    "            beta_t = self.beta(t)\n",
    "            f = -0.5 * beta_t * x\n",
    "            g = torch.sqrt(torch.tensor(beta_t, device=device))\n",
    "            \n",
    "            score = self.score(x, t_tensor)\n",
    "            \n",
    "            drift = f - g**2 * score\n",
    "            \n",
    "            noise = torch.randn_like(x) if i < n_steps - 1 else 0\n",
    "            x = x + drift * dt + g * torch.sqrt(torch.abs(torch.tensor(dt))) * noise\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_ode(self, shape, n_steps=1000):\n",
    "        \"\"\"\n",
    "        Sample using the Probability Flow ODE (deterministic).\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        x = torch.randn(shape, device=device)\n",
    "        \n",
    "        dt = -self.T / n_steps\n",
    "        \n",
    "        for i in range(n_steps):\n",
    "            t = self.T - i * self.T / n_steps\n",
    "            t_tensor = torch.full((shape[0],), t, device=device)\n",
    "            \n",
    "            beta_t = self.beta(t)\n",
    "            f = -0.5 * beta_t * x\n",
    "            g_squared = beta_t\n",
    "            \n",
    "            score = self.score(x, t_tensor)\n",
    "            \n",
    "            drift = f - 0.5 * g_squared * score\n",
    "            \n",
    "            x = x + drift * dt\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape, method='sde', n_steps=1000):\n",
    "        \"\"\"Unified sampling interface.\"\"\"\n",
    "        if method == 'sde':\n",
    "            return self.sample_sde(shape, n_steps)\n",
    "        elif method == 'ode':\n",
    "            return self.sample_ode(shape, n_steps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {method}\")\n",
    "\n",
    "\n",
    "def train_sde(model, optimizer, scheduler, dataloader, epochs, device, per_epoch_callback=None):\n",
    "    \"\"\"Training loop for continuous-time SDE model.\"\"\"\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training SDE Model\")\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            loss_history.append(loss.item())\n",
    "            \n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"⠀{loss.item():12.4f}\",\n",
    "                epoch=f\"{epoch+1}/{epochs}\",\n",
    "                lr=f\"{scheduler.get_last_lr()[0]:.2E}\"\n",
    "            )\n",
    "            progress_bar.update()\n",
    "        \n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(model)\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpsde_unet = ScoreNet(lambda t: torch.ones(1).to(device)).to(device)\n",
    "vpsde_model = VPSDE(vpsde_unet, beta_min=0.1, beta_max=20.0, T=1.0).to(device)\n",
    "\n",
    "vpsde_optimizer = torch.optim.Adam(vpsde_model.parameters(), lr=learning_rate)\n",
    "vpsde_scheduler = torch.optim.lr_scheduler.ExponentialLR(vpsde_optimizer, 0.9999)\n",
    "\n",
    "def vpsde_reporter(model):\n",
    "    \"\"\"Callback to visualize generated samples during training.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        nsamples = 10\n",
    "        \n",
    "        samples = model.sample((nsamples, 28*28), method='ode', n_steps=200).cpu()\n",
    "        samples = (samples + 1) / 2\n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "        \n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=nsamples)\n",
    "        plt.figure(figsize=(15, 2))\n",
    "        plt.gca().set_axis_off()\n",
    "        plt.imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        plt.title(\"VP-SDE Samples (ODE)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "comparison_epochs_d3 = 50\n",
    "comparison_batch_size_d3 = 256\n",
    "\n",
    "comparison_dataloader_d3 = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=comparison_batch_size_d3,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "ddpm_d3 = DDPM(\n",
    "    ScoreNet(lambda t: torch.ones(1).to(device)), \n",
    "    T=T\n",
    ").to(device)\n",
    "\n",
    "ddpm_d3_opt = torch.optim.Adam(ddpm_d3.parameters(), lr=learning_rate)\n",
    "ddpm_d3_sch = torch.optim.lr_scheduler.ExponentialLR(ddpm_d3_opt, 0.9999)\n",
    "\n",
    "loss_ddpm_d3 = train(\n",
    "    ddpm_d3,\n",
    "    ddpm_d3_opt,\n",
    "    ddpm_d3_sch,\n",
    "    comparison_dataloader_d3,\n",
    "    epochs=comparison_epochs_d3,\n",
    "    device=device,\n",
    "    ema=True,\n",
    "    per_epoch_callback=None,\n",
    ")\n",
    "\n",
    "vpsde_d3 = VPSDE(\n",
    "    ScoreNet(lambda t: torch.ones(1).to(device)).to(device),\n",
    "    beta_min=0.1,\n",
    "    beta_max=20.0,\n",
    "    T=1.0\n",
    ").to(device)\n",
    "\n",
    "vpsde_d3_opt = torch.optim.Adam(vpsde_d3.parameters(), lr=learning_rate)\n",
    "vpsde_d3_sch = torch.optim.lr_scheduler.ExponentialLR(vpsde_d3_opt, 0.9999)\n",
    "\n",
    "loss_vpsde_d3 = train_sde(\n",
    "    vpsde_d3,\n",
    "    vpsde_d3_opt,\n",
    "    vpsde_d3_sch,\n",
    "    comparison_dataloader_d3,\n",
    "    epochs=comparison_epochs_d3,\n",
    "    device=device,\n",
    "    per_epoch_callback=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Plot 1: Training loss comparison with smoothing\n",
    "window = 100\n",
    "ddpm_d3_smooth = np.convolve(loss_ddpm_d3, np.ones(window)/window, mode='valid')\n",
    "vpsde_d3_smooth = np.convolve(loss_vpsde_d3, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(ddpm_d3_smooth, label=\"DDPM\", linewidth=2, alpha=0.8)\n",
    "axes[0].plot(vpsde_d3_smooth, label=\"Continuous-Time SDE\", linewidth=2, alpha=0.8)\n",
    "axes[0].set_title(\"Training Loss Comparison\", fontsize=11)\n",
    "axes[0].set_xlabel(\"Training Step\", fontsize=10)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=10)\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: DDPM samples\n",
    "ddpm_d3.eval()\n",
    "with torch.no_grad():\n",
    "    ddpm_d3_samples = ddpm_d3.sample((16, 28 * 28)).cpu()\n",
    "    ddpm_d3_samples = ((ddpm_d3_samples + 1) / 2).clamp(0, 1)\n",
    "    ddpm_d3_grid = utils.make_grid(ddpm_d3_samples.reshape(-1, 1, 28, 28), nrow=4, padding=1, pad_value=1.0)\n",
    "\n",
    "axes[1].imshow(transforms.functional.to_pil_image(ddpm_d3_grid), cmap=\"gray\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"DDPM Samples\", fontsize=11)\n",
    "\n",
    "# Plot 3: Continuous-Time SDE samples (using ODE for deterministic generation)\n",
    "vpsde_d3.eval()\n",
    "with torch.no_grad():\n",
    "    vpsde_d3_samples = vpsde_d3.sample((16, 28 * 28), method='ode', n_steps=500).cpu()\n",
    "    vpsde_d3_samples = ((vpsde_d3_samples + 1) / 2).clamp(0, 1)\n",
    "    vpsde_d3_grid = utils.make_grid(vpsde_d3_samples.reshape(-1, 1, 28, 28), nrow=4, padding=1, pad_value=1.0)\n",
    "\n",
    "axes[2].imshow(transforms.functional.to_pil_image(vpsde_d3_grid), cmap=\"gray\")\n",
    "axes[2].axis(\"off\")\n",
    "axes[2].set_title(\"Continuous-Time SDE Samples\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure Mode Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Numerical instability when beta(t) grows too quickly\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Test different beta schedules (varying max beta)\n",
    "beta_configs = [\n",
    "    (0.1, 5.0, \"Stable (0.1, 5.0)\"),\n",
    "    (0.1, 20.0, \"Standard (0.1, 20.0)\"),\n",
    "    (0.1, 50.0, \"High (0.1, 50.0)\"),\n",
    "    (0.1, 100.0, \"Very High (0.1, 100.0)\"),\n",
    "    (0.1, 200.0, \"Extreme (0.1, 200.0)\"),\n",
    "    (0.1, 500.0, \"Unstable (0.1, 500.0)\"),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (beta_min_test, beta_max_test, label) in enumerate(beta_configs):\n",
    "    test_model = VPSDE(\n",
    "        ScoreNet(lambda t: torch.ones(1).to(device)).to(device),\n",
    "        beta_min=beta_min_test,\n",
    "        beta_max=beta_max_test,\n",
    "        T=1.0\n",
    "    ).to(device)\n",
    "    \n",
    "    test_model._network.load_state_dict(vpsde_d3._network.state_dict())\n",
    "    \n",
    "    test_model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            samples = test_model.sample((16, 28 * 28), method='ode', n_steps=500).cpu()\n",
    "            samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "            grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "            \n",
    "            axes[i].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "            axes[i].axis(\"off\")\n",
    "            axes[i].set_title(label)\n",
    "        except Exception as e:\n",
    "            axes[i].text(0.5, 0.5, f\"Failed:\\n{str(e)[:50]}\", \n",
    "                        ha='center', va='center', fontsize=8)\n",
    "            axes[i].axis(\"off\")\n",
    "            axes[i].set_title(label + \" (Failed)\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Effect of Beta Schedule Growth Rate on Numerical Stability\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Miscalibrated noise leading to over-smooth or noisy samples\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Test different initial noise scales\n",
    "noise_scales = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "vpsde_d3.eval()\n",
    "for i, scale in enumerate(noise_scales):\n",
    "    with torch.no_grad():\n",
    "        device_vp = next(vpsde_d3.parameters()).device\n",
    "        \n",
    "        x = torch.randn((16, 28 * 28), device=device_vp) * scale\n",
    "        \n",
    "        dt = -vpsde_d3.T / 500\n",
    "        \n",
    "        for step in range(500):\n",
    "            t = vpsde_d3.T - step * vpsde_d3.T / 500\n",
    "            t_tensor = torch.full((16,), t, device=device_vp)\n",
    "            \n",
    "            beta_t = vpsde_d3.beta(t)\n",
    "            f = -0.5 * beta_t * x\n",
    "            g_squared = beta_t\n",
    "            \n",
    "            score = vpsde_d3.score(x, t_tensor)\n",
    "            \n",
    "            drift = f - 0.5 * g_squared * score\n",
    "            \n",
    "            x = x + drift * dt\n",
    "        \n",
    "        samples = x.cpu()\n",
    "        samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "        \n",
    "        axes[i].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"Initial noise scale={scale:.1f}\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Effect of Miscalibrated Initial Noise on Sample Quality\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction 4: Classifier-Free Guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalScoreNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A conditional version of ScoreNet that can take class labels as input.\n",
    "    Supports classifier-free guidance by conditioning on labels or null token.\n",
    "    \"\"\"\n",
    "    def __init__(self, marginal_prob_std, num_classes=10, channels=[32, 64, 128, 256], embed_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_embed = nn.Sequential(\n",
    "            GaussianFourierProjection(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        \n",
    "        self.class_embed = nn.Embedding(num_classes + 1, embed_dim)\n",
    "        \n",
    "        # Encoding layers\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "        self.dense1 = Dense(embed_dim * 2, channels[0]) \n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "        self.dense2 = Dense(embed_dim * 2, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense3 = Dense(embed_dim * 2, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "        self.dense4 = Dense(embed_dim * 2, channels[3])\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
    "        \n",
    "        # Decoding layers\n",
    "        self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense5 = Dense(embed_dim * 2, channels[2])\n",
    "        self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        \n",
    "        self.tconv3 = nn.ConvTranspose2d(\n",
    "            channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1\n",
    "        )\n",
    "        self.dense6 = Dense(embed_dim * 2, channels[1])\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        \n",
    "        self.tconv2 = nn.ConvTranspose2d(\n",
    "            channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1\n",
    "        )\n",
    "        self.dense7 = Dense(embed_dim * 2, channels[0])\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "        \n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
    "        \n",
    "        self.act = lambda x: x * torch.sigmoid(x)\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "    \n",
    "    def forward(self, x, t, y):\n",
    "        \"\"\"\n",
    "        Forward pass with conditioning.\n",
    "        \"\"\"\n",
    "        t_embed = self.act(self.time_embed(t))\n",
    "        y_embed = self.class_embed(y)\n",
    "        \n",
    "        embed = torch.cat([t_embed, y_embed], dim=-1)\n",
    "        \n",
    "        # Encoding path\n",
    "        h1 = self.conv1(x)\n",
    "        h1 += self.dense1(embed)\n",
    "        h1 = self.gnorm1(h1)\n",
    "        h1 = self.act(h1)\n",
    "        \n",
    "        h2 = self.conv2(h1)\n",
    "        h2 += self.dense2(embed)\n",
    "        h2 = self.gnorm2(h2)\n",
    "        h2 = self.act(h2)\n",
    "        \n",
    "        h3 = self.conv3(h2)\n",
    "        h3 += self.dense3(embed)\n",
    "        h3 = self.gnorm3(h3)\n",
    "        h3 = self.act(h3)\n",
    "        \n",
    "        h4 = self.conv4(h3)\n",
    "        h4 += self.dense4(embed)\n",
    "        h4 = self.gnorm4(h4)\n",
    "        h4 = self.act(h4)\n",
    "        \n",
    "        # Decoding path\n",
    "        h = self.tconv4(h4)\n",
    "        h += self.dense5(embed)\n",
    "        h = self.tgnorm4(h)\n",
    "        h = self.act(h)\n",
    "        \n",
    "        h = self.tconv3(torch.cat([h, h3], dim=1))\n",
    "        h += self.dense6(embed)\n",
    "        h = self.tgnorm3(h)\n",
    "        h = self.act(h)\n",
    "        \n",
    "        h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "        h += self.dense7(embed)\n",
    "        h = self.tgnorm2(h)\n",
    "        h = self.act(h)\n",
    "        \n",
    "        h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "        \n",
    "        h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "        return h\n",
    "\n",
    "\n",
    "class ClassifierFreeDDPM(nn.Module):\n",
    "    \"\"\"\n",
    "    DDPM with Classifier-Free Guidance.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, num_classes=10, T=1000, beta_1=1e-4, beta_T=2e-2, \n",
    "                 p_uncond=0.1, guidance_scale=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._network = network\n",
    "        self.num_classes = num_classes\n",
    "        self.null_class = num_classes\n",
    "        self.p_uncond = p_uncond \n",
    "        self.guidance_scale = guidance_scale\n",
    "        \n",
    "        self.network = lambda x, t, y: (\n",
    "            self._network(x.reshape(-1, 1, 28, 28), (t.squeeze() / T), y)\n",
    "        ).reshape(-1, 28 * 28)\n",
    "        \n",
    "        self.T = T\n",
    "        \n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_1, beta_T, T + 1))\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
    "    \n",
    "    def forward_diffusion(self, x0, t, epsilon):\n",
    "        \"\"\"Forward diffusion: q(x_t | x_0)\"\"\"\n",
    "        mean = torch.sqrt(self.alpha_bar[t]) * x0\n",
    "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
    "        return mean + std * epsilon\n",
    "    \n",
    "    def reverse_diffusion(self, xt, t, y, epsilon):\n",
    "        \"\"\"\n",
    "        Reverse diffusion with classifier-free guidance.\n",
    "        \"\"\"\n",
    "        eps_cond = self.network(xt, t, y)\n",
    "        \n",
    "        y_uncond = torch.full_like(y, self.null_class)\n",
    "        eps_uncond = self.network(xt, t, y_uncond)\n",
    "        \n",
    "        eps_guided = eps_uncond + self.guidance_scale * (eps_cond - eps_uncond)\n",
    "        \n",
    "        mean = (\n",
    "            1.0 / torch.sqrt(self.alpha[t]) * (\n",
    "                xt - (self.beta[t]) / torch.sqrt(1 - self.alpha_bar[t]) * eps_guided\n",
    "            )\n",
    "        )\n",
    "        std = torch.where(\n",
    "            t > 0,\n",
    "            torch.sqrt(((1 - self.alpha_bar[t - 1]) / (1 - self.alpha_bar[t])) * self.beta[t]),\n",
    "            0,\n",
    "        )\n",
    "        \n",
    "        return mean + std * epsilon\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape, y=None, guidance_scale=None):\n",
    "        \"\"\"\n",
    "        Sample from the model with classifier-free guidance.\n",
    "        \"\"\"\n",
    "        device = self.beta.device\n",
    "        nsamples = shape[0]\n",
    "        \n",
    "        if y is None:\n",
    "            y = torch.randint(0, self.num_classes, (nsamples,), device=device)\n",
    "        \n",
    "        old_guidance_scale = self.guidance_scale\n",
    "        if guidance_scale is not None:\n",
    "            self.guidance_scale = guidance_scale\n",
    "        \n",
    "        xT = torch.randn(shape).to(device)\n",
    "        xt = xT\n",
    "        \n",
    "        for t in range(self.T, 0, -1):\n",
    "            noise = torch.randn_like(xT) if t > 1 else 0\n",
    "            t_tensor = torch.tensor(t).expand(xt.shape[0], 1).to(device)\n",
    "            xt = self.reverse_diffusion(xt, t_tensor, y, noise)\n",
    "        \n",
    "        self.guidance_scale = old_guidance_scale\n",
    "        \n",
    "        return xt\n",
    "    \n",
    "    def loss(self, x0, y):\n",
    "        \"\"\"\n",
    "        Training loss with classifier-free guidance.\n",
    "        Randomly drops conditioning during training.\n",
    "        \"\"\"\n",
    "        batch_size = x0.shape[0]\n",
    "        device = x0.device\n",
    "        \n",
    "        t = torch.randint(1, self.T, (batch_size, 1)).to(device)\n",
    "        \n",
    "        epsilon = torch.randn_like(x0)\n",
    "        \n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "        \n",
    "        mask = torch.rand(batch_size, device=device) < self.p_uncond\n",
    "        y_train = y.clone()\n",
    "        y_train[mask] = self.null_class\n",
    "        \n",
    "        epsilon_pred = self.network(xt, t, y_train)\n",
    "        \n",
    "        return F.mse_loss(epsilon_pred, epsilon, reduction=\"mean\")\n",
    "\n",
    "\n",
    "def train_cfg(model, optimizer, scheduler, dataloader, epochs, device, per_epoch_callback=None):\n",
    "    \"\"\"Training loop for classifier-free guidance model.\"\"\"\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training CFG Model\")\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            loss_history.append(loss.item())\n",
    "            \n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"⠀{loss.item():12.4f}\",\n",
    "                epoch=f\"{epoch+1}/{epochs}\",\n",
    "                lr=f\"{scheduler.get_last_lr()[0]:.2E}\"\n",
    "            )\n",
    "            progress_bar.update()\n",
    "        \n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(model)\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "comparison_epochs_d4 = 50\n",
    "comparison_batch_size_d4 = 256\n",
    "\n",
    "comparison_dataloader_d4 = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=comparison_batch_size_d4,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Train standard DDPM (unconditional)\n",
    "ddpm_d4 = DDPM(\n",
    "    ScoreNet(lambda t: torch.ones(1).to(device)), \n",
    "    T=T\n",
    ").to(device)\n",
    "\n",
    "ddpm_d4_opt = torch.optim.Adam(ddpm_d4.parameters(), lr=learning_rate)\n",
    "ddpm_d4_sch = torch.optim.lr_scheduler.ExponentialLR(ddpm_d4_opt, 0.9999)\n",
    "\n",
    "loss_ddpm_d4 = train(\n",
    "    ddpm_d4,\n",
    "    ddpm_d4_opt,\n",
    "    ddpm_d4_sch,\n",
    "    comparison_dataloader_d4,\n",
    "    epochs=comparison_epochs_d4,\n",
    "    device=device,\n",
    "    ema=True,\n",
    "    per_epoch_callback=None,\n",
    ")\n",
    "\n",
    "# Train Classifier-Free Guidance DDPM\n",
    "cfg_network = ConditionalScoreNet(\n",
    "    lambda t: torch.ones(1).to(device),\n",
    "    num_classes=10\n",
    ").to(device)\n",
    "\n",
    "cfg_model = ClassifierFreeDDPM(\n",
    "    cfg_network,\n",
    "    num_classes=10,\n",
    "    T=T,\n",
    "    p_uncond=0.1,  # 10% unconditional training\n",
    "    guidance_scale=2.0\n",
    ").to(device)\n",
    "\n",
    "cfg_opt = torch.optim.Adam(cfg_model.parameters(), lr=learning_rate)\n",
    "cfg_sch = torch.optim.lr_scheduler.ExponentialLR(cfg_opt, 0.9999)\n",
    "\n",
    "loss_cfg_d4 = train_cfg(\n",
    "    cfg_model,\n",
    "    cfg_opt,\n",
    "    cfg_sch,\n",
    "    comparison_dataloader_d4,\n",
    "    epochs=comparison_epochs_d4,\n",
    "    device=device,\n",
    "    per_epoch_callback=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Plot 1: Training loss comparison with smoothing\n",
    "window = 100\n",
    "ddpm_d4_smooth = np.convolve(loss_ddpm_d4, np.ones(window)/window, mode='valid')\n",
    "cfg_d4_smooth = np.convolve(loss_cfg_d4, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(ddpm_d4_smooth, label=\"DDPM\", linewidth=2, alpha=0.8)\n",
    "axes[0].plot(cfg_d4_smooth, label=\"Classifier-Free Guidance\", linewidth=2, alpha=0.8)\n",
    "axes[0].set_title(\"Training Loss Comparison\", fontsize=11)\n",
    "axes[0].set_xlabel(\"Training Step\", fontsize=10)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=10)\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: DDPM samples (unconditional)\n",
    "ddpm_d4.eval()\n",
    "with torch.no_grad():\n",
    "    ddpm_d4_samples = ddpm_d4.sample((16, 28 * 28)).cpu()\n",
    "    ddpm_d4_samples = ((ddpm_d4_samples + 1) / 2).clamp(0, 1)\n",
    "    ddpm_d4_grid = utils.make_grid(ddpm_d4_samples.reshape(-1, 1, 28, 28), nrow=4, padding=1, pad_value=1.0)\n",
    "\n",
    "axes[1].imshow(transforms.functional.to_pil_image(ddpm_d4_grid), cmap=\"gray\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"DDPM Samples\", fontsize=11)\n",
    "\n",
    "# Plot 3: CFG samples (conditional - each row is a different class)\n",
    "cfg_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Generate samples for classes 0-3 (4 samples each)\n",
    "    y_cfg = torch.tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3], device=device)\n",
    "    cfg_d4_samples = cfg_model.sample((16, 28 * 28), y=y_cfg, guidance_scale=2.0).cpu()\n",
    "    cfg_d4_samples = ((cfg_d4_samples + 1) / 2).clamp(0, 1)\n",
    "    cfg_d4_grid = utils.make_grid(cfg_d4_samples.reshape(-1, 1, 28, 28), nrow=4, padding=1, pad_value=1.0)\n",
    "\n",
    "axes[2].imshow(transforms.functional.to_pil_image(cfg_d4_grid), cmap=\"gray\")\n",
    "axes[2].axis(\"off\")\n",
    "axes[2].set_title(\"CFG Samples (classes 0-3)\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure Mode Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Effect of guidance scale on sample quality and diversity\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Test different guidance scales\n",
    "guidance_scales = [0.0, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cfg_model.eval()\n",
    "for i, scale in enumerate(guidance_scales):\n",
    "    with torch.no_grad():\n",
    "        # Generate samples for class 3 (easier to see quality differences)\n",
    "        y_test = torch.full((16,), 3, dtype=torch.long, device=device)\n",
    "        samples = cfg_model.sample((16, 28 * 28), y=y_test, guidance_scale=scale).cpu()\n",
    "        samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "        \n",
    "        axes[i].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"Guidance scale={scale:.1f}\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Effect of Guidance Scale on Sample Quality\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Insufficient unconditional training (p_uncond too low)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Test different unconditional training probabilities\n",
    "p_uncond_values = [0.0, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "training_epochs_cfg = 20\n",
    "\n",
    "for i, p_uncond_val in enumerate(p_uncond_values):\n",
    "    # Train a new model with different p_uncond\n",
    "    cfg_net_test = ConditionalScoreNet(\n",
    "        lambda t: torch.ones(1).to(device),\n",
    "        num_classes=10\n",
    "    ).to(device)\n",
    "    \n",
    "    cfg_test = ClassifierFreeDDPM(\n",
    "        cfg_net_test,\n",
    "        num_classes=10,\n",
    "        T=T,\n",
    "        p_uncond=p_uncond_val,\n",
    "        guidance_scale=2.0\n",
    "    ).to(device)\n",
    "    \n",
    "    cfg_opt_test = torch.optim.Adam(cfg_test.parameters(), lr=learning_rate)\n",
    "    cfg_sch_test = torch.optim.lr_scheduler.ExponentialLR(cfg_opt_test, 0.9999)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    dataloader_cfg_test = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "        batch_size=comparison_batch_size_d4,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    _ = train_cfg(\n",
    "        cfg_test,\n",
    "        cfg_opt_test,\n",
    "        cfg_sch_test,\n",
    "        dataloader_cfg_test,\n",
    "        epochs=training_epochs_cfg,\n",
    "        device=device,\n",
    "        per_epoch_callback=None,\n",
    "    )\n",
    "    \n",
    "    cfg_test.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test = torch.full((16,), 7, dtype=torch.long, device=device)\n",
    "        samples = cfg_test.sample((16, 28 * 28), y=y_test, guidance_scale=2.0).cpu()\n",
    "        samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "        \n",
    "        axes[i].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"p_uncond={p_uncond_val:.2f}\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Effect of Unconditional Training Probability\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Class-specific quality variation (some classes harder to generate)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate samples for all 10 classes\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cfg_model.eval()\n",
    "for class_idx in range(10):\n",
    "    with torch.no_grad():\n",
    "        y_class = torch.full((16,), class_idx, dtype=torch.long, device=device)\n",
    "        samples = cfg_model.sample((16, 28 * 28), y=y_class, guidance_scale=2.0).cpu()\n",
    "        samples = ((samples + 1) / 2).clamp(0, 1)\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=4)\n",
    "        \n",
    "        axes[class_idx].imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        axes[class_idx].axis(\"off\")\n",
    "        axes[class_idx].set_title(f\"Class {class_idx}\")\n",
    "\n",
    "plt.suptitle(\"Failure Mode: Class-Specific Quality Variation\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison: Baseline vs. Four Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features before final classification layer\"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "def train_classifier(epochs=5):\n",
    "    classifier = MNISTClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"./mnist_data\", download=True, train=True, \n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5,), (0.5,))\n",
    "                      ])),\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    classifier.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = classifier(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Classifier training epoch {epoch+1}/{epochs} complete\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def calculate_inception_score(samples, classifier, splits=10):\n",
    "    \"\"\"\n",
    "    Calculate Inception Score for generated samples.\n",
    "    Higher is better. Measures quality and diversity.\n",
    "    \"\"\"\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = F.softmax(classifier(samples), dim=1).cpu().numpy()\n",
    "        \n",
    "        split_scores = []\n",
    "        for k in range(splits):\n",
    "            part = preds[k * (len(preds) // splits): (k + 1) * (len(preds) // splits), :]\n",
    "            py = np.mean(part, axis=0)\n",
    "            scores = []\n",
    "            for i in range(part.shape[0]):\n",
    "                pyx = part[i, :]\n",
    "                scores.append(entropy(pyx, py))\n",
    "            split_scores.append(np.exp(np.mean(scores)))\n",
    "        \n",
    "        return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "def calculate_fid(real_features, fake_features):\n",
    "    \"\"\"\n",
    "    Calculate Fréchet Inception Distance.\n",
    "    Lower is better. Measures similarity to real distribution.\n",
    "    \"\"\"\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "def calculate_quality_score(samples, classifier):\n",
    "    \"\"\"\n",
    "    Measures average confidence of classifier on generated samples.\n",
    "    Higher is better.\n",
    "    \"\"\"\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = F.softmax(classifier(samples), dim=1)\n",
    "        max_probs = preds.max(dim=1)[0]\n",
    "        return max_probs.mean().item(), max_probs.std().item()\n",
    "\n",
    "def calculate_diversity_score(samples, classifier):\n",
    "    \"\"\"\n",
    "    Measures diversity by entropy of predicted class distribution.\n",
    "    Higher is better (max ~2.3 for uniform over 10 classes).\n",
    "    \"\"\"\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = F.softmax(classifier(samples), dim=1)\n",
    "        avg_pred = preds.mean(dim=0).cpu().numpy()\n",
    "        return entropy(avg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Classifier for Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier = train_classifier(epochs=5)\n",
    "\n",
    "real_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, \n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((0.5,), (0.5,))\n",
    "                  ])),\n",
    "    batch_size=1000,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "real_images = next(iter(real_loader))[0].to(device)\n",
    "with torch.no_grad():\n",
    "    real_features = mnist_classifier.get_features(real_images).cpu().numpy()\n",
    "\n",
    "print(f\"Classifier trained. Real features shape: {real_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate and Evaluate All Five Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_T = 1000\n",
    "comparison_lr = 1e-3\n",
    "comparison_epochs = 50\n",
    "comparison_batch_size = 256\n",
    "num_eval_samples = 1000\n",
    "\n",
    "comparison_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=comparison_batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1: Baseline DDPM\n",
    "print(\"\\n[1/5] Training Baseline DDPM...\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "baseline_model = DDPM(\n",
    "    ScoreNet(lambda t: torch.ones(1).to(device)), \n",
    "    T=comparison_T\n",
    ").to(device)\n",
    "\n",
    "baseline_opt = torch.optim.Adam(baseline_model.parameters(), lr=comparison_lr)\n",
    "baseline_sch = torch.optim.lr_scheduler.ExponentialLR(baseline_opt, 0.9999)\n",
    "\n",
    "baseline_loss = train(\n",
    "    baseline_model,\n",
    "    baseline_opt,\n",
    "    baseline_sch,\n",
    "    comparison_dataloader,\n",
    "    epochs=comparison_epochs,\n",
    "    device=device,\n",
    "    ema=True,\n",
    "    per_epoch_callback=None,\n",
    ")\n",
    "\n",
    "baseline_model.eval()\n",
    "with torch.no_grad():\n",
    "    baseline_samples = baseline_model.sample((num_eval_samples, 28 * 28)).cpu()\n",
    "    baseline_samples = ((baseline_samples + 1) / 2).clamp(0, 1)\n",
    "    baseline_samples_img = baseline_samples.reshape(-1, 1, 28, 28)\n",
    "\n",
    "    baseline_samples_norm = (baseline_samples_img - 0.5) / 0.5\n",
    "    baseline_samples_norm = baseline_samples_norm.to(device)\n",
    "    \n",
    "    is_mean, is_std = calculate_inception_score(baseline_samples_norm, mnist_classifier)\n",
    "    fake_features = mnist_classifier.get_features(baseline_samples_norm).cpu().numpy()\n",
    "    fid = calculate_fid(real_features, fake_features)\n",
    "    quality_mean, quality_std = calculate_quality_score(baseline_samples_norm, mnist_classifier)\n",
    "    diversity = calculate_diversity_score(baseline_samples_norm, mnist_classifier)\n",
    "    \n",
    "    results['Baseline DDPM'] = {\n",
    "        'IS': f\"{is_mean:.3f} ± {is_std:.3f}\",\n",
    "        'FID': f\"{fid:.2f}\",\n",
    "        'Quality': f\"{quality_mean:.3f} ± {quality_std:.3f}\",\n",
    "        'Diversity': f\"{diversity:.3f}\",\n",
    "        'Final Loss': f\"{baseline_loss[-1]:.4f}\"\n",
    "    }\n",
    "\n",
    "print(f\"Baseline DDPM - IS: {is_mean:.3f}, FID: {fid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2: Direction 1 - Flow Matching\n",
    "print(\"\\n[2/5] Training Flow Matching Model...\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "fm_model = FlowMatchingModel(\n",
    "    ScoreNet(lambda t: torch.ones(1).to(device)).to(device)\n",
    ").to(device)\n",
    "\n",
    "fm_opt = torch.optim.Adam(fm_model.parameters(), lr=comparison_lr)\n",
    "fm_sch = torch.optim.lr_scheduler.ExponentialLR(fm_opt, 0.9999)\n",
    "\n",
    "fm_loss = train_flow_matching(\n",
    "    fm_model,\n",
    "    fm_opt,\n",
    "    fm_sch,\n",
    "    comparison_dataloader,\n",
    "    epochs=comparison_epochs,\n",
    "    device=device,\n",
    "    per_epoch_callback=None,\n",
    ")\n",
    "\n",
    "fm_model.eval()\n",
    "with torch.no_grad():\n",
    "    fm_samples = fm_model.sample(nsamples=num_eval_samples, n_steps=50).cpu()\n",
    "    fm_samples = ((fm_samples + 1) / 2).clamp(0, 1)\n",
    "    fm_samples_img = fm_samples.reshape(-1, 1, 28, 28)\n",
    "    fm_samples_norm = (fm_samples_img - 0.5) / 0.5\n",
    "    fm_samples_norm = fm_samples_norm.to(device)\n",
    "    \n",
    "    is_mean, is_std = calculate_inception_score(fm_samples_norm, mnist_classifier)\n",
    "    fake_features = mnist_classifier.get_features(fm_samples_norm).cpu().numpy()\n",
    "    fid = calculate_fid(real_features, fake_features)\n",
    "    quality_mean, quality_std = calculate_quality_score(fm_samples_norm, mnist_classifier)\n",
    "    diversity = calculate_diversity_score(fm_samples_norm, mnist_classifier)\n",
    "    \n",
    "    results['Direction 1: Flow Matching'] = {\n",
    "        'IS': f\"{is_mean:.3f} ± {is_std:.3f}\",\n",
    "        'FID': f\"{fid:.2f}\",\n",
    "        'Quality': f\"{quality_mean:.3f} ± {quality_std:.3f}\",\n",
    "        'Diversity': f\"{diversity:.3f}\",\n",
    "        'Final Loss': f\"{fm_loss[-1]:.4f}\"\n",
    "    }\n",
    "\n",
    "print(f\"Flow Matching - IS: {is_mean:.3f}, FID: {fid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3: Direction 2 - Latent Diffusion Model\n",
    "print(\"\\n[3/5] Training Latent Diffusion Model...\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "ae_latent_dim = 32\n",
    "ae_epochs = 30\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim=ae_latent_dim).to(device)\n",
    "ae_opt = torch.optim.Adam(autoencoder.parameters(), lr=comparison_lr)\n",
    "\n",
    "for epoch in range(ae_epochs):\n",
    "    autoencoder.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(comparison_dataloader):\n",
    "        data = data.to(device)\n",
    "        ae_opt.zero_grad()\n",
    "        reconstructed, _ = autoencoder(data)\n",
    "        data_img = data.reshape(-1, 1, 28, 28)\n",
    "        loss = F.mse_loss(reconstructed, data_img)\n",
    "        loss.backward()\n",
    "        ae_opt.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Autoencoder epoch {epoch+1}/{ae_epochs}, Loss: {total_loss/len(comparison_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_ddpm = LatentDDPM(\n",
    "    autoencoder,\n",
    "    LatentUNet(latent_dim=ae_latent_dim).to(device),\n",
    "    T=comparison_T\n",
    ").to(device)\n",
    "\n",
    "latent_opt = torch.optim.Adam(latent_ddpm.network.parameters(), lr=comparison_lr)\n",
    "latent_sch = torch.optim.lr_scheduler.ExponentialLR(latent_opt, 0.9999)\n",
    "\n",
    "latent_loss = train_latent_ddpm(\n",
    "    latent_ddpm,\n",
    "    latent_opt,\n",
    "    latent_sch,\n",
    "    comparison_dataloader,\n",
    "    epochs=comparison_epochs,\n",
    "    device=device,\n",
    "    per_epoch_callback=None,\n",
    ")\n",
    "\n",
    "latent_ddpm.eval()\n",
    "with torch.no_grad():\n",
    "    latent_samples = latent_ddpm.sample(num_eval_samples).cpu()\n",
    "    latent_samples = ((latent_samples + 1) / 2).clamp(0, 1)\n",
    "    latent_samples_img = latent_samples.reshape(-1, 1, 28, 28)\n",
    "    latent_samples_norm = (latent_samples_img - 0.5) / 0.5\n",
    "    latent_samples_norm = latent_samples_norm.to(device)\n",
    "    \n",
    "    is_mean, is_std = calculate_inception_score(latent_samples_norm, mnist_classifier)\n",
    "    fake_features = mnist_classifier.get_features(latent_samples_norm).cpu().numpy()\n",
    "    fid = calculate_fid(real_features, fake_features)\n",
    "    quality_mean, quality_std = calculate_quality_score(latent_samples_norm, mnist_classifier)\n",
    "    diversity = calculate_diversity_score(latent_samples_norm, mnist_classifier)\n",
    "    \n",
    "    results['Direction 2: Latent Diffusion'] = {\n",
    "        'IS': f\"{is_mean:.3f} ± {is_std:.3f}\",\n",
    "        'FID': f\"{fid:.2f}\",\n",
    "        'Quality': f\"{quality_mean:.3f} ± {quality_std:.3f}\",\n",
    "        'Diversity': f\"{diversity:.3f}\",\n",
    "        'Final Loss': f\"{latent_loss[-1]:.4f}\"\n",
    "    }\n",
    "\n",
    "print(f\"Latent Diffusion - IS: {is_mean:.3f}, FID: {fid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 4: Direction 3 - Continuous-Time SDE (VP-SDE)\n",
    "print(\"\\n[4/5] Training VP-SDE Model...\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "vpsde_model = VPSDE(\n",
    "    ScoreNet(lambda t: torch.ones(1).to(device)),\n",
    "    beta_min=0.1,\n",
    "    beta_max=20.0,\n",
    "    T=1.0\n",
    ").to(device)\n",
    "\n",
    "vpsde_opt = torch.optim.Adam(vpsde_model.parameters(), lr=comparison_lr)\n",
    "vpsde_sch = torch.optim.lr_scheduler.ExponentialLR(vpsde_opt, 0.9999)\n",
    "\n",
    "vpsde_loss = train_sde(\n",
    "    vpsde_model,\n",
    "    vpsde_opt,\n",
    "    vpsde_sch,\n",
    "    comparison_dataloader,\n",
    "    epochs=comparison_epochs,\n",
    "    device=device,\n",
    "    per_epoch_callback=None,\n",
    ")\n",
    "\n",
    "vpsde_model.eval()\n",
    "with torch.no_grad():\n",
    "    vpsde_samples = vpsde_model.sample((num_eval_samples, 28 * 28), n_steps=100).cpu()\n",
    "    vpsde_samples = ((vpsde_samples + 1) / 2).clamp(0, 1)\n",
    "    vpsde_samples_img = vpsde_samples.reshape(-1, 1, 28, 28)\n",
    "    vpsde_samples_norm = (vpsde_samples_img - 0.5) / 0.5\n",
    "    vpsde_samples_norm = vpsde_samples_norm.to(device)\n",
    "    \n",
    "    is_mean, is_std = calculate_inception_score(vpsde_samples_norm, mnist_classifier)\n",
    "    fake_features = mnist_classifier.get_features(vpsde_samples_norm).cpu().numpy()\n",
    "    fid = calculate_fid(real_features, fake_features)\n",
    "    quality_mean, quality_std = calculate_quality_score(vpsde_samples_norm, mnist_classifier)\n",
    "    diversity = calculate_diversity_score(vpsde_samples_norm, mnist_classifier)\n",
    "    \n",
    "    results['Direction 3: VP-SDE'] = {\n",
    "        'IS': f\"{is_mean:.3f} ± {is_std:.3f}\",\n",
    "        'FID': f\"{fid:.2f}\",\n",
    "        'Quality': f\"{quality_mean:.3f} ± {quality_std:.3f}\",\n",
    "        'Diversity': f\"{diversity:.3f}\",\n",
    "        'Final Loss': f\"{vpsde_loss[-1]:.4f}\"\n",
    "    }\n",
    "\n",
    "print(f\"VP-SDE - IS: {is_mean:.3f}, FID: {fid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 5: Direction 4 - Classifier-Free Guidance\n",
    "print(\"\\n[5/5] Training Classifier-Free Guidance Model...\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "labeled_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=comparison_batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "cfg_network = ConditionalScoreNet(\n",
    "    marginal_prob_std=lambda t: torch.ones(1).to(device),\n",
    "    num_classes=10\n",
    ").to(device)\n",
    "\n",
    "cfg_model = ClassifierFreeDDPM(\n",
    "    cfg_network,\n",
    "    num_classes=10,\n",
    "    T=comparison_T,\n",
    "    p_uncond=0.1,\n",
    "    guidance_scale=2.0\n",
    ").to(device)\n",
    "\n",
    "cfg_opt = torch.optim.Adam(cfg_model.parameters(), lr=comparison_lr)\n",
    "cfg_sch = torch.optim.lr_scheduler.ExponentialLR(cfg_opt, 0.9999)\n",
    "\n",
    "cfg_loss = train_cfg(\n",
    "    cfg_model,\n",
    "    cfg_opt,\n",
    "    cfg_sch,\n",
    "    labeled_dataloader,\n",
    "    epochs=comparison_epochs,\n",
    "    device=device,\n",
    "    per_epoch_callback=None,\n",
    ")\n",
    "\n",
    "cfg_model.eval()\n",
    "with torch.no_grad():\n",
    "    samples_per_class = num_eval_samples // 10\n",
    "    all_samples = []\n",
    "    \n",
    "    for class_idx in range(10):\n",
    "        class_labels = torch.full((samples_per_class,), class_idx, dtype=torch.long).to(device)\n",
    "        class_samples = cfg_model.sample(\n",
    "            (samples_per_class, 28 * 28),\n",
    "            y=class_labels,\n",
    "            guidance_scale=2.0\n",
    "        ).cpu()\n",
    "        all_samples.append(class_samples)\n",
    "    \n",
    "    cfg_samples = torch.cat(all_samples, dim=0)\n",
    "    cfg_samples = ((cfg_samples + 1) / 2).clamp(0, 1)\n",
    "    cfg_samples_img = cfg_samples.reshape(-1, 1, 28, 28)\n",
    "    cfg_samples_norm = (cfg_samples_img - 0.5) / 0.5\n",
    "    cfg_samples_norm = cfg_samples_norm.to(device)\n",
    "    \n",
    "    is_mean, is_std = calculate_inception_score(cfg_samples_norm, mnist_classifier)\n",
    "    fake_features = mnist_classifier.get_features(cfg_samples_norm).cpu().numpy()\n",
    "    fid = calculate_fid(real_features, fake_features)\n",
    "    quality_mean, quality_std = calculate_quality_score(cfg_samples_norm, mnist_classifier)\n",
    "    diversity = calculate_diversity_score(cfg_samples_norm, mnist_classifier)\n",
    "    \n",
    "    results['Direction 4: Classifier-Free Guidance'] = {\n",
    "        'IS': f\"{is_mean:.3f} ± {is_std:.3f}\",\n",
    "        'FID': f\"{fid:.2f}\",\n",
    "        'Quality': f\"{quality_mean:.3f} ± {quality_std:.3f}\",\n",
    "        'Diversity': f\"{diversity:.3f}\",\n",
    "        'Final Loss': f\"{cfg_loss[-1]:.4f}\"\n",
    "    }\n",
    "\n",
    "print(f\"Classifier-Free Guidance - IS: {is_mean:.3f}, FID: {fid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results).T\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bffd3855f5744f588d5be1e5c4aed3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b61ee9c62994863b718c086d4182f44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "645d91e4bb974b1196be61b5077c9dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78dc714c7aa347fb9fc41abf420222d9",
       "IPY_MODEL_c1260f271df547fbb2a158ff6b3a3ff4",
       "IPY_MODEL_e7313fdbb70442f4867644dfc85c3bcc"
      ],
      "layout": "IPY_MODEL_a501588b5eb0494996dfb136565365ca"
     }
    },
    "78dc714c7aa347fb9fc41abf420222d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89c68eded05d441daf94d145addb5ece",
      "placeholder": "​",
      "style": "IPY_MODEL_2bffd3855f5744f588d5be1e5c4aed3e",
      "value": "Training:  24%"
     }
    },
    "89c68eded05d441daf94d145addb5ece": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b905c5b2ad846ca837bd20cce2bf094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a501588b5eb0494996dfb136565365ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aca161ff9f4b4a20b1457a8ee864f150": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1416c32c4af4fe9a3c3fdcc5f33aca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1260f271df547fbb2a158ff6b3a3ff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b61ee9c62994863b718c086d4182f44",
      "max": 5900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b905c5b2ad846ca837bd20cce2bf094",
      "value": 1394
     }
    },
    "e7313fdbb70442f4867644dfc85c3bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "placeholder": "​",
      "style": "IPY_MODEL_aca161ff9f4b4a20b1457a8ee864f150",
      "value": " 1393/5900 [05:15&lt;16:04,  4.67it/s, epoch=12/50, loss=⠀   2400.1270]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
